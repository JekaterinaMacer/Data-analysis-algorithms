# Data analysis algorithms

Урок 8. Снижение размерности данных


## Summary

* Снижать размерность данных можно за счет
    * отбора признаков (корреляция, взаимная информация, переборные методы, вес коэфф. регрессии, feature_importances и др.)
    * понижения размерности (случайные проекции, PCA, ICA (Independent Component Analysis), NMF (Non-negative Matrix Factorization) и др.)
* Уменьшение размерности 
    * ускоряет работу моделей
    * улучшает интерпретируемость решения
    * улучшает точность модели, если были удалены шумовые и нерелевантные признаки
* В основе PCA используется понятие [_собственного вектора_](https://ru.wikipedia.org/wiki/%D0%A1%D0%BE%D0%B1%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80) - это вектор, умножение которого на матрицу даёт коллинеарный вектор - тот же вектор, умноженный на некоторое число, называемое _собственным значением_

### Определения
*Снижение размерности*

**Снижение размерности** — это преобразование данных, состоящее в уменьшении числа переменных.

**Одномерный отбор признаков** — оценка предсказательной силы каждого признака (насколько он коррелирует с целевой переменной).

**Корреляция** — статистическая взаимосвязь двух или более случайных величин.

**Взаимная информация** — статистическая функция двух случайных величин, описывающая количество информации, содержащееся в одной случайной величине относительно другой.

**Понижение размерности** — это преобразование данных, состоящее в уменьшении числа переменных путём получения новых переменных.

**Метод главных компонент** — один из основных способов уменьшить размерность данных, потеряв наименьшее количество информации. 
